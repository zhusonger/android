# 录制核心模块


  编辑者  |  版本号  |  日期       |   说明
---------|---------|------------|----------
   朱凇   |   1.0   | 2016.11.16 | 初次录制功能模块文档
   朱凇   |	  1.1   | 2017.4.18  | 增加音频源切换处理与断线重连优化

   
	
---

## 录制核心总览
录制核心总共包括4部分，视频的录制编码，音频的录制编码，本地视频文件合成，直播流数据上传。视频/音频录制是公有部分，本地视频合成与直播上传只是负责使用录制产生的数据。

视频录制拆分出了单独的库项目，录制相关的代码都在这个库中，对于需要扩展的应用自定义部分，可以继承RecordService实现自定义的部分，RecordService目前也已经开放的大部分需要的功能。

最初的版本，音频、视频都是在一个线程中完成的，这样会出现一个问题，因为我们需要OpenGl进行中间画面的处理，这样在一些GPU调度算法不是很好的设备上，会在视频那一步消耗较长时间，阻塞了音频等其它操作，导致音频出现卡断的情况，目前的实现是音频和视频的生成与编码是独立的线程，彼此没有关系，他们的作用只是负责数据的生成，可以理解为生产/消费者模式中的生产者。而MuxThread和RTMPThread是消费者，使用worker线程产生的数据。


### 音频的参数
* 采样率 44100HZ
* 声道 单声道/双声道
* 音频格式 PCM_16BIT/PCM_8BIT/PCM_20BIT

> 这三个参数决定了1s采集的数据量。
> 
> Sample : 44100HZ, 单声道, PCM_16BIT
> 
> 每秒采样数据 : 44100 * 1 * 16/8 = 88200byte/s

---

通过AudioRecord可以获取支持当前手机设备支持的最小音频数据缓存大小,一般会根据我们定义的每次采样的大小进行对齐。

```
	 int minBufferSize = AudioRecord.getMinBufferSize(sampleRate, channel, format);
	 int bufferSize = (int) Math.ceil((float) minBufferSize / samplePerFrame) * samplePerFrame;
	 
```
这里就是每次从AudioRecord读取出来的每次采样的数据量大小，比如4096byte

---

那么每次采样的时间间隔的理论时间间隔为: 4096 / 88200 =0.046400s=46.4ms=46400us

所以我们每次在获取到音频数据后，压入到编码器中的时间戳是自己计算的时间，保证音频时间戳的连续性。

编码器取出的时间戳就是我们压入的数据的时间戳。

---

### 视频的参数

* 长 1280
* 宽 720
* 桢率 24fps
* 关键帧 2fps
* 色彩格式 COLOR_FormatSurface / COLOR_Format32bitABGR8888

那么每秒的数据量为: 1280 * 720 * 24 * 32/8

---

编码器传入比特率的参数，就是希望编码器以传入的格式，尽可能的接近传入的比特率，所以我们可以直接使用比特率来计算每秒的数据传输量，比如2000kps的比特率:

2000*1000 / 8 /1024 = 244kb/s 

表示如果希望能流畅的传输视频数据，就需要上传速度达到244kb/s能保证视频质量，不过并不是每秒都需要这么大的传输量，当画面不动时，数据量就较少。

---

### 对带宽的要求
根据视频与音频的计算，可以知道保证直播正常的带宽要求，音频是固定的，经过编码器编码之后，数据量更小，大概在6kb/s左右，视频的在244kb/s，那么总的就是250kb/s，这是高清的情况下的带宽要求。其它清晰度都可以计算出来。

---

* __ 录制中心实现如下图所示： __

![core_img][record_core_img]

参考地址 : <http://blog.csdn.net/jinzhuojun/article/details/32163149>

### 1.视频的录制编码

__ 核心类 : ScreenCapture & VideoWorker & VideoEncoderCore __

* 视频录制区分系统
	1. [Android5.0, Android5.0+]
	2. [Android4.4,Android5.0)
	
#### 生成视频数据[ScreenCapture]

* Andorid5.0+ : 发起请求,获取MediaProjection对象，通过MediaProjection创建虚拟显示对象，创建VirtualDisplay会传入希望的宽高，输出到的surface

* Android4.4+ : 使用4.4提供的adb录屏的代码，编译成可执行文件，进行屏幕的录制，需要传入的信息跟Android5.0一样。

```
@TargetApi(Build.VERSION_CODES.LOLLIPOP)
private void createVirtualDisplay(Surface surface) {
		if (isAND_V5()) {
			mVirtualDisplay = mMediaProjection.createVirtualDisplay(DEFAULT_DISPLAY_NAME,
					mTargetWidth, mTargetHeight, mDpi, DisplayManager.VIRTUAL_DISPLAY_FLAG_PUBLIC,
					surface, null, null);
		} else if (isAND_V4_4() && MODE_GL_OES == mWorkMode) {
			if (null != mOnCreateVirtualDisplayListener) {
				mOnCreateVirtualDisplayListener.onCreateVirtualDisplay(surface);
			}
		}
	}
```
#### 修改视频数据[ScreenCapture]
* 在生成视频画面后，经过了opengl的处理，可以对画面进行自定义的处理，比如我们的隐私模式,截屏等，就是通过这种方式实现的。目前我们是在循环获取视频数据的时候去调用处理方法_processCapturedFrame_的，这个方法的作用有补桢，丢帧，画面处理等。
 当前我们使用的模式是gl_oes

```
	public void processCapturedFrame(long consumerPts) {
		if (!mIsInited)
			return;

		if (MODE_GL_2D == mWorkMode) {
			processCapturedFrame_GL_2D();
		} else if (MODE_GL_OES == mWorkMode) {
			processCapturedFrame_GL_OES(consumerPts);
		}
	}
	
```


#### 编码视频数据
* 编码的过程也在系统的VirtualDisplay中处理了，VirtualDisplay传入的surface就是编码器的surface,生成的画面直接输出到传入的surface上。

#### 取出编码数据[VideoEncoderCore]
* 通过MediaCodec获取编码之后的视频数据，加入到数据队列。INFO_OUTPUT_FORMAT_CHANGED标志视频格式信息，直播或者本地录制，最开始都要使用这个对象的数据。

```
	public long drainEncoder(boolean endOfStream) throws Exception {
        if (endOfStream) {
            mEncoder.signalEndOfInputStream();
        }

        long lastEncoderPtsNs = 0;
        while (true) {
            MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
            int bufferIndex = mEncoder.dequeueOutputBuffer(info, 0);
            ByteBuffer[] outputBuffers = mEncoder.getOutputBuffers();
            if (bufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                outputBuffers = mEncoder.getOutputBuffers();
            } else if(bufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat mediaFormat = mEncoder.getOutputFormat();
                if (null != mCallback) {
                    mCallback.onStatus(RecordCallback.ENCODE_INFO_OUTPUT_FORMAT_CHANGED, mediaFormat);
                }
            } else if (bufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                if (!endOfStream) {
                    break;
                } else {
                    Log.d(TAG, "VideoEncoderCore.drainEncoder : wait for eos");
                }
            } else if (bufferIndex < 0) {
                Log.w(TAG, "VideoEncoderCore.drainEncoder : bufferIndex < 0");
            } else if (bufferIndex >= 0) {
                ByteBuffer data = outputBuffers[bufferIndex];
                if (null != data) {
                    data.position(info.offset);
                    data.limit(info.offset + info.size);
                }

                if (null != mCallback) {
                    mCallback.onStatus(RecordCallback.ENCODE_OUTPUT_DATA, data, info);
                }
                lastEncoderPtsNs = info.presentationTimeUs * 1000;
                mEncoder.releaseOutputBuffer(bufferIndex, false);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    if(!endOfStream) {
                        Log.w(TAG, "VideoEncoderCore.drainEncoder : reached end of stream unexpectedly");
                    } else {
                        Log.d(TAG, "VideoEncoderCore.drainEncoder : end of stream reached");
                    }
                    break;
                }
            }
        }

        return lastEncoderPtsNs;

    }
    
```

### 2.音频的录制编码
__ 核心类 : AudioWorker & AudioEncoderCore __

#### 生成音频裸数据pcm[AudioEncoderCore]
* 通过AudioRecord开启录音，设置一些录音的参数，如采样率，声道，音频格式等，开启音频录制，从编码器读取一块缓存，通过recorder读取出音频数据，然后再塞入编码器。

```
	public void offerEncoder(AudioRecord record, boolean endOfStream)
            throws Exception {
        try {
            int bufferIndex = mEncoder.dequeueInputBuffer(0);
            if (bufferIndex >= 0) {
                ByteBuffer[] inputBuffers = mEncoder.getInputBuffers();
                ByteBuffer bufferCache = inputBuffers[bufferIndex];
                int audioSize = record.read(bufferCache, bufferCache.remaining());

                if (audioSize == AudioRecord.ERROR_INVALID_OPERATION
                        || audioSize == AudioRecord.ERROR_BAD_VALUE) {
                    Log.w(TAG, "offerEncoder : error audioSize = "+audioSize);
                } else {
                    int flag = endOfStream ? MediaCodec.BUFFER_FLAG_END_OF_STREAM : 0;

                    ／*mEncoder.queueInputBuffer(bufferIndex, 0, audioSize, mLastPresentationTimeUs, flag);

                    // calculate interval for actual audio buffer size once
                    if (presentationInterval == 0) {
                        presentationInterval = (int)((float)audioSize / sampleByteSizeInSec * 1000000);
                    }
                    mLastPresentationTimeUs += presentationInterval;*/
                    
                    // 1.1 update
                    long now = System.nanoTime() / 1000;
					// adjust timestamp
					if (changeAudioSource) {
						changeAudioSource = false;
						// update presentationInterval, audio size maybe changed
						presentationInterval = (int)((float)audioSize / sampleByteSizeInSec * 1000000);
						long nowFrameEncodeEndTimeUs = now + presentationInterval;
						// change audio source, and block time
						if (nowFrameEncodeEndTimeUs > mLastFrameEncodeEndTimeUs) {
							mLastPresentationTimeUs += (nowFrameEncodeEndTimeUs - mLastFrameEncodeEndTimeUs);
						}
					}
					mEncoder.queueInputBuffer(bufferIndex, 0, audioSize, mLastPresentationTimeUs, flag);

					// calculate interval for actual audio buffer size once
					if (presentationInterval == 0) {
						presentationInterval = (int)((float)audioSize / sampleByteSizeInSec * 1000000);
					}

					mLastPresentationTimeUs += presentationInterval;
					mLastFrameEncodeEndTimeUs = now + presentationInterval;
                }
            } else if (endOfStream) {
                unExpectedEndOfStream = true;
            }
        } catch (Exception e) {
            if (null != mCallback) {
                mCallback.onStatus(RecordCallback.ENCODE_OFFER_ERROR, e);
            }
        }

    }
    
```

#### 编码音频数据
* 在上一步塞入编码器之后，编码器就会替我们完成音频编码的工作，生成字节较小的音频数据。

#### 取出编码数据[AudioEncoderCore]
* 通过编码器取出编码后的音频数据，塞入到数据队列。

```
	public void drainEncoder(boolean endOfStream) throws Exception {
        while(true) {
            MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
            int bufferIndex = mEncoder.dequeueOutputBuffer(info, 0);


            ByteBuffer[] buffers = mEncoder.getOutputBuffers();
            if (bufferIndex == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                MediaFormat format = mEncoder.getOutputFormat();
                if (null != mCallback) {
                    mCallback.onStatus(RecordCallback.ENCODE_INFO_OUTPUT_FORMAT_CHANGED, format);
                }
            } else if (bufferIndex == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                buffers = mEncoder.getOutputBuffers();
            } else if (bufferIndex == MediaCodec.INFO_TRY_AGAIN_LATER) {
                // no available data, break
                if (!endOfStream || unExpectedEndOfStream) {
                    break;
                } else {
                    // wait to end
                }
            } else if (bufferIndex < 0) {
                Log.w(TAG, "AudioEncoderCore.drainEncoder : bufferIndex < 0 ");
            } else {
                ByteBuffer data = buffers[bufferIndex]; // data after encode
                if (null != data) {
                    data.position(info.offset);
                    data.limit(info.offset + info.size);
                }

                if (null != mCallback) {
                    mCallback.onStatus(RecordCallback.ENCODE_OUTPUT_DATA, data, info);
                }
                mEncoder.releaseOutputBuffer(bufferIndex, false);

                if ((info.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    if(!endOfStream) {
                        Log.w(TAG, "AudioEncoderCore.drainEncoder : reached end of stream unexpectedly");
                    } else {
                        Log.d(TAG, "AudioEncoderCore.drainEncoder : end of stream reached");
                    }
                    break;
                }
            }
        }

```


### 3.本地视频合成
__ 核心类 : RecordService & MuxThread __

* 本地视频合成是通过使用android提供的MediaMuxer类，通过该工具类，把1，2步编码后的数据有序的写入到MediaMuxer中，在1，2步骤中获取到数据时，有描述音视频数据格式的MediaFormat，MediaMuxer的addTrack方法，可以得到音视频对应的轨道id，通过writeSampleData把音视频数据写入文件，开始录制文件调用MediaMuxer的start方法，结束调用stop&release方法，如果没有正确的stop，那么生成的文件是不完整的。

#### 开启本地录制线程[RecordService]
* 创建MediaMuxer对象，并开启1，2步的过程，即启动AudioWorker&VideoWorker线程，这2个线程实现音视频的生成与编码，并把生成的编码后的数据加入到队列中。在worker线程的回调方法中，获取到音视频的MediaFormat，加入到MediaMuxer获取写入音视频数据的轨道id.

```
	public boolean startLocalRecord(String outFile) {

        if (!checkRecord(outFile)) {
            return false;
        }

        File file = new File(outFile);
        // delete old file
        if (file.exists()) {
            file.delete();
        }
        File parentFile = file.getParentFile();
        if (null != parentFile) {
            parentFile.mkdirs();
        }

        try {
            muxer = new MediaMuxer(outFile, MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);
        } catch (IOException e) {
            e.printStackTrace();
            return false;
        }

	    startForeground();

        startService(new Intent(this, getClass()));
        audioWorker.start();
        videoWorker.start();

        new MuxThread(outFile).start();

        mPrepareFlag = mPrepareFlag | START_BIT;
	    startTimestamp = System.currentTimeMillis();
	    videoPath = outFile;
        return true;
    }

```

```
	audioTrack = muxer.addTrack(format);
	videoTrack = muxer.addTrack(format);

```

#### 写入音视频数据[MuxThread]
* 在worker线程，把编码后的数据加入的队列，在MuxThread中，我们只负责使用数据，写入编码好的数据到MediaMuxer中，同时做了sd卡空间的监测，因为写入本地的一个问题，就是存储空间的大小,当空间不足时，需要完成录制，避免没有正确结束视频，导致视频文件不完整。

```
		@Override
        public void run() {
            super.run();
            Log.d(TAG, "MuxThread.run : start");
            synchronized (mMuxReadyFence) {
                while(mCurrentFlag != FLAG_MUX_ALL
		                && !Thread.currentThread().isInterrupted()) {
                    try{
                        mMuxReadyFence.wait(100);
                    } catch (InterruptedException e) {
	                    Thread.currentThread().interrupt();
                    }
                }
            }

	        int code = 0;
	        int audioEncodeSize = mAudioData.size();
	        // there is only an format data,
	        if (audioEncodeSize <= 1 || mCurrentFlag != FLAG_MUX_ALL) {
		        int videoFlag = mCurrentFlag & 3;
		        if (videoFlag == OFF) {
			        code |= CODE_WRITE_VIDEO_ERROR;
		        }

		        int audioFlag = mCurrentFlag & 12;
		        if (audioFlag == OFF) {
			        code |= CODE_WRITE_AUDIO_ERROR;
		        }
		        code |= CODE_STOP_MUX_ERROR;
	        }

	        Log.d(TAG, "MuxThread.run : start "+ mCurrentFlag
			        +", code = " + code);
	        if (code == 0) {
		        muxer.start();

		        afterMuxStart(path);

		        String sdPath = Environment.getExternalStorageDirectory().getPath();
		        while (!Thread.currentThread().isInterrupted()) {
			        EncodeOutputData audio = mAudioData.poll();
			        EncodeOutputData video = mVideoData.poll();
			        if (null != audio) {
				        ByteBuffer buffer = ByteBuffer.wrap(audio.data);
				        MediaCodec.BufferInfo info = audio.info;
				        // muxer don't need config, the presentation is invalid
				        if ((info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0) {
					        buffer = null;
				        }
				        if (null != buffer) {
					        buffer.position(info.offset);
					        buffer.limit(info.offset + info.size);
					        try {
						        muxer.writeSampleData(audio.track, buffer, info);
						        long pts = info.presentationTimeUs / 1000;
						        if (info.flags != MediaCodec.BUFFER_FLAG_END_OF_STREAM) {
							        consumeAudio = pts;
						        }
						        audioSize += info.size;
					        } catch (Exception e) {
						        code |= CODE_WRITE_AUDIO_ERROR;
						        Thread.currentThread().interrupt();
					        }
				        }
			        }

			        if (null != video) {
				        ByteBuffer buffer = ByteBuffer.wrap(video.data);
				        MediaCodec.BufferInfo info = video.info;
				        // muxer don't need config, the presentation is invalid
				        if ((info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0) {
					        buffer = null;
				        }
				        if (null != buffer) {
					        buffer.position(info.offset);
					        buffer.limit(info.offset + info.size);
					        try {
						        muxer.writeSampleData(video.track, buffer, info);
						        long pts = info.presentationTimeUs / 1000;
						        if (info.flags != MediaCodec.BUFFER_FLAG_END_OF_STREAM) {
							        consumeVideo = pts;
						        }
						        videoSize += info.size;
					        } catch (Exception e) {
						        code |= CODE_WRITE_VIDEO_ERROR;
						        Thread.currentThread().interrupt();
					        }
				        }
			        }


			        if(mCurrentFlag == FLAG_MUX_NONE) {
				        break;
			        }

			        StatFs fs = new StatFs(sdPath);
			        long freeByte = fs.getAvailableBytes();
			        if (freeByte <= MIN_FREE_BYTE) {
				        code |= CODE_NO_FREE_BYTE;
				        break;
			        }

			        int audioDataSize = mAudioData.size();
			        int videoDataSize = mVideoData.size();
			        if (audioDataSize == 0 && videoDataSize == 0) {
				        try {
					        Thread.sleep(2);
				        } catch (InterruptedException e) {
					        Thread.currentThread().interrupt();
				        }
			        }
		        }

		        Log.d(TAG, "MuxThread.run: stop "+mCurrentFlag);
		        try{
			        muxer.stop();
			        muxer.release();
		        } catch (Exception e) {
			        e.printStackTrace();
			        code |= CODE_STOP_MUX_ERROR;
		        }
	        }


	        afterMuxStop(code, path, audioSize, videoSize, consumeAudio, consumeVideo);

	        if (isRunning() && code > 0) {
		        stopRecord();
	        }
        }
        
```

### 4.直播rtmp上传
__ 核心类 : RecordService & RTMPThread & RTMPDump __

#### 开启音视频录制[RecordService]
同本地录制一样，启动AudioWorker&VideoWorker线程，让这2个线程产生音视频数据，储存到队列中，提供给rtmp直播使用。然后开启RTMPThread进行数据的上传。

```
	public boolean startOnline(String url, int sourceId) {
        if (!checkRecord(url)) {
            return false;
        }
	    this.sourceId = sourceId;

	    startForeground();
        startService(new Intent(this, getClass()));
        audioWorker.start();
        videoWorker.start();

        new RTMPThread(url).start();

        mPrepareFlag = mPrepareFlag | START_BIT;
        return true;
    }

```

#### 连接RTMP地址[RTMPThread & RTMPDump & ConnectReceiver]

* 使用开源库rtmp，实现音视频数据的上传，RTMP底层是TCP协议，用socket实现，是一种实时消息传输协议，是直播中较常用的一种传输协议。在RTMPThread线程启动后，通过RTMPDump封装的接口调用connectServer，连接rtmp服务器。

* RTMP连接发送音视频数据，首先需要发送音视频的头信息，否则服务端无法解析音视频数据。

* 为了让直播更加稳定，加入了断线重连的功能，就是通过ConnectReceiver监听系统广播，监听到网络状态变化之后，如果网络从断开到重新连接上，重新连接rtmp服务器，并发送音视频头信息。

__ 连接RTMP __

```
boolean success = RTMPDump.getInstance().jConnectServer(url);
if (!success) {
	Log.w(TAG, "RTMPThread.run : connectServer["+url+"] is fail");	stopRecord();
	return;
} d

```
__ 重连RTMP __

```
	/*private class ConnectReceiver extends BroadcastReceiver {

		private volatile RTMPThread rtmpThread;
		protected ConnectReceiver() {
		}

		@Override
		public void onReceive(Context context, Intent intent) {

			if (rtmpThread == null) {
				return;
			}

			ConnectivityManager manager = (ConnectivityManager) context.getSystemService(Context.CONNECTIVITY_SERVICE);
			NetworkInfo activeInfo = manager.getActiveNetworkInfo();
			if (null == activeInfo || !activeInfo.isAvailable()) {
				rtmpThread.isNetBroken = true;
				Log.e(TAG, "rtmpThread.isNetBroken = true");
			} else {
				if (rtmpThread.isNetBroken) {
					Log.d(TAG, "rtmpThread.isNetBroken = false, reconnect...");
					RTMPDump.getInstance().disconnect();
					boolean ret = RTMPDump.getInstance().jConnectServer(rtmpThread.url);
					if (!ret) {
						return;
					}
					// reconnect success, send audio/video config data
					EncodeOutputData audio = rtmpThread.audioHeaderData;
					if (null != audio) {
						RTMPDump.getInstance().sendHeader(audio.data, RTMPDump.AUDIO_DATA, audio.info.size);
					}

					EncodeOutputData video = rtmpThread.videoHeaderData;
					if (null != video) {
						RTMPDump.getInstance().sendHeader(video.data, RTMPDump.VIDEO_DATA, video.info.size);
					}

					// update net broken flag
					rtmpThread.isNetBroken = false;
					Log.d(TAG, "rtmpThread.isNetBroken = false, reconnect success ");
				}
			}
		}
	}*/
		
	
```

#### 发送音视频数据
* 在RTMPThread线程中，发送音视频数据。在第一次发送数据出错时，会纪录下首次出错时间，如果1分钟内，还是有发送错误，就会断开直播，否则如果在5s内发送数据正常，就会重置首次出错时间。重新开始计算断线超时时间。
* 还有就是积压包的问题，如果积压包个数超过了最大值，就会计算积压包的数据量，如果超过限制大小，就会清空worker线程产生的数据。

```
		while (!mStop) {
				/*ret = sendAudio();
				if (RTMP_SUCCESS != ret) {
					rtmpErrorACount++;
					lastRtmpErrorTime = System.currentTimeMillis();
					if (firstRtmpErrorTime <= 0) {
						firstRtmpErrorTime = lastRtmpErrorTime;
					}

					if (isRtmpError(firstRtmpErrorTime)) {
						isRtmpError = true;
						break;
					}

					blockThread();
					continue;
				}

				ret = sendVideo();
				if (RTMP_SUCCESS != ret) {
					rtmpErrorVCount++;
					lastRtmpErrorTime = System.currentTimeMillis();
					if (firstRtmpErrorTime <= 0) {
						firstRtmpErrorTime = lastRtmpErrorTime;
					}

					if (isRtmpError(firstRtmpErrorTime)) {
						isRtmpError = true;
						break;
					}

					blockThread();
					continue;
				}*/
				
				// update 1.1
				if(mCurrentFlag == FLAG_MUX_NONE) {
					break;
				}
				isRtmpError = false;
				isAudioError = false;
				if (isRtmpBroken) {
					boolean netBroken = true;
					try {
						HttpURLConnection conn = (HttpURLConnection)new URL("https://www.baidu.com").openConnection();
						int netCode = conn.getResponseCode();
						conn.disconnect();
						Log.d(TAG, "connect to https://www.baidu.com, code = "+code);
						netBroken = netCode != 200;
					} catch (Exception e) {
						Log.e(TAG, "connect to https://www.baidu.com error");
					}

					// 网络异常
					if (netBroken) {
						// 已经是超过了重连的时间间隔,断开连接
						if (isRtmpError(firstRtmpErrorTime)) {
							isRtmpError = true;
							break;
						}
						// 否则阻塞1s，重新查看网络是否正常
						blockThread(1000);
						continue;
					}

					// 网络正常, 进行重练
					Log.d(TAG, "rtmpThread reconnect...");
					RTMPDump.getInstance().disconnect();
					boolean connectRet = RTMPDump.getInstance().jConnectServer(rtmpThread.url);
					if (!connectRet) {
						Log.e(TAG, "rtmpThread reconnect failure, stop it");
						mStop = true;
						continue;
					}
					// reconnect success, send audio/video config data
					EncodeOutputData audio = audioHeaderData;
					int headerRet = 0;
					if (null != audio) {
						headerRet = RTMPDump.getInstance().sendHeader(audio.data, RTMPDump.AUDIO_DATA, audio.info.size);
					}
					if (RTMP_SUCCESS != headerRet) {
						isRtmpError = true;
						Log.e(TAG, "rtmpThread send Audio Header failure, stop it");
						break;
					}
					EncodeOutputData video = videoHeaderData;
					if (null != video) {
						headerRet = RTMPDump.getInstance().sendHeader(video.data, RTMPDump.VIDEO_DATA, video.info.size);
					}
					if (RTMP_SUCCESS != headerRet) {
						isRtmpError = true;
						Log.e(TAG, "rtmpThread send Video Header failure, stop it");
						break;
					}
					isRtmpBroken = false;
					long brokenTimeMs = System.nanoTime() / 1000000 - firstRtmpErrorTime;
					firstRtmpErrorTime = 0;
					Log.d(TAG, "rtmpThread reconnect success, blockTimeMs = " + brokenTimeMs
							+ " audio = "+ rtmpErrorACount+","
							+ " video = "+ rtmpErrorVCount);
				}

				if (consumeVideo < consumeAudio) {
					ret = sendVideo();
					if (RTMP_SUCCESS != ret) {
						isRtmpBroken = true;
						rtmpErrorVCount++;
						lastRtmpErrorTime = System.nanoTime() / 1000000;
						if (firstRtmpErrorTime <= 0) {
							firstRtmpErrorTime = lastRtmpErrorTime;
						}
						continue;
					}
				} else {
					ret = sendAudio();
					if (RTMP_SUCCESS != ret) {
						if (ret == CODE_NET_BROKEN) {
							isRtmpBroken = true;
							rtmpErrorACount++;
							lastRtmpErrorTime = System.nanoTime() / 1000000;
							if (firstRtmpErrorTime <= 0) {
								firstRtmpErrorTime = lastRtmpErrorTime;
							}
						} else if(ret == CODE_AUDIO_ERROR){
							isAudioError = true;
							Log.e(TAG, "rtmpThread send Audio empty in "+MAX_AUDIO_EMPTY_TIME / 1000+"s, stop it");
							break;
						}
						continue;
					}
				}

				if(mCurrentFlag == FLAG_MUX_NONE) {
					break;
				}

				// recover net block status
				if (firstRtmpErrorTime > 0 && !isNetBroken && isRtmpNormally(lastRtmpErrorTime)) {
					Log.w(TAG, "RTMPThread.run: rtmp error occur, but recover in "+ MAX_RTMP_NORMALLY_TIME /1000+"s :"
							+ " audio = "+ rtmpErrorACount+","
							+ " video = "+ rtmpErrorVCount+","
							+ " first error time = "+firstRtmpErrorTime+","
							+ " last error time =  "+lastRtmpErrorTime);

					isRtmpError = false;
					firstRtmpErrorTime = 0;
				}

				int audioDataSize = mAudioData.size();
				int videoDataSize = mVideoData.size();
				if (audioDataSize == 0 && videoDataSize == 0) {
					blockThread();
				}
				// out of abandon size, release it
				else if (videoDataSize >= abandonVideoMaxSize){
					long abandonByte = 0;
					for (int i = 0; i < videoDataSize; i++) {
						EncodeOutputData data = mVideoData.peek();
						abandonByte += data.data.length;
					}

					// keyframe will offer immediately
					if (abandonByte >= ABANDON_VIDEO_FRAME_SIZE_LIMIT) {
						Log.d(TAG, "abandonByte = "+abandonByte+"byte, clear");
						mVideoData.clear();
						mAudioData.clear();
					}
				}
			}

```

---

[record_core_img]:record_core.png

